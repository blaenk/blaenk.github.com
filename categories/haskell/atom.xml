<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: haskell | Jorge Israel Peña]]></title>
  <link href="http://blaenkdenum.com/categories/haskell/atom.xml" rel="self"/>
  <link href="http://blaenkdenum.com/"/>
  <updated>2013-05-06T22:12:07-07:00</updated>
  <id>http://blaenkdenum.com/</id>
  <author>
    <name><![CDATA[Jorge Israel Peña]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Naive Convolution in Haskell]]></title>
    <link href="http://blaenkdenum.com/posts/naive-convolution-in-haskell/"/>
    <updated>2013-01-04T18:21:00-08:00</updated>
    <id>http://blaenkdenum.com/posts/naive-convolution-in-haskell</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#principle">Principle</a></li>
  <li><a href="#definition">Definition</a>    <ul>
      <li><a href="#convolution-machine">Convolution Machine</a></li>
    </ul>
  </li>
  <li><a href="#implementation">Implementation</a>    <ul>
      <li><a href="#padding">Padding</a></li>
      <li><a href="#lets-roll">Let’s Roll</a></li>
    </ul>
  </li>
  <li><a href="#reduction">Reduction</a></li>
  <li><a href="#parallelization">Parallelization</a>    <ul>
      <li><a href="#parmap">parMap</a></li>
      <li><a href="#benchmark">Benchmark</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a>    <ul>
      <li><a href="#imperative-approach">Imperative Approach</a></li>
      <li><a href="#optimizations">Optimizations</a></li>
    </ul>
  </li>
</ul>

<p><a href="http://en.wikipedia.org/wiki/Convolution">Convolution</a> is a mathematical method of combining two signals to form a third signal. Passing the <a href="http://en.wikipedia.org/wiki/Dirac_delta_function">Dirac delta function</a> (unit impulse) <script type="math/tex">\delta[n]</script> through a linear system results in the impulse response <script type="math/tex">h[n]</script>. The impulse response is simply the signal resulting from passing the unit impulse (Dirac delta function) through a linear system.</p>

<h2 id="principle">Principle</h2>

<p>The properties of <a href="http://www.cns.nyu.edu/~david/handouts/linear-systems/linear-systems.html">homogeneity</a> and <a href="http://en.wikipedia.org/wiki/Shift-invariant_system">shift-invariance</a> in <a href="http://en.wikipedia.org/wiki/LTI_system_theory">Linear Time-Invariant System Theory</a> holds that scaling and shifting the input results in the same scaling and shifting in the output. Because of these properties, we can represent any impulse as a shifted and scaled delta function and consequently know what the impulse response will be for that scaled and shifted impulse.</p>

<p>An impulse of <script type="math/tex">-3</script> at the <script type="math/tex">8^{th}</script> sample would be represented as a unit impulse by scaling the delta function by <script type="math/tex">-3</script> and shifting it to the right by <script type="math/tex">8</script> samples: <script type="math/tex">-3\delta[n-8]</script>, where <script type="math/tex">n-8</script> means the <script type="math/tex">8^{th}</script> sample is now the <script type="math/tex">0^{th}</script>. Due to homogeneity and shift invariance, we can determine the impulse response of this impulse by simply scaling and shifting the unit impulse response in the same manner. In other words:</p>

<script type="math/tex; mode=display">-3\delta[n-8] \mapsto -3h[n-8]</script>

<p>What this means is that if we know the unit impulse response of a system, we consequently know how the system will react to <em>any</em> impulse. These impulse responses can then be synthesized to form the output signal that would result from running the input signal through the actual system. An example of the powerful implications of this method is <a href="http://en.wikipedia.org/wiki/Convolution_reverb">convolution reverb</a>, in which an impulse response of a physical or virtual space is generated and then convolved with any input signal to simulate the effect of reverberation in that space.</p>

<p>In short, the input signal <em>convolved</em> with the unit impulse response results in the output signal. Convolution of input signal <script type="math/tex">x[n]</script> with unit impulse <script type="math/tex">h[n]</script> to generate output signal <script type="math/tex">y[n]</script> is denoted as:</p>

<script type="math/tex; mode=display">x[n] * h[n] = y[n]</script>

<p>Since convolution allows us to go from input signal <script type="math/tex">x[n]</script> to output signal <script type="math/tex">y[n]</script>, we can conclude that convolution involves the generation of the impulse response for each impulse in the input signal as decomposed by <a href="http://www.dspguide.com/ch5/7.htm">impulse decomposition</a>, <em>as well as</em> the subsequent synthesis of each impulse response, to generate the output signal.</p>

<h2 id="definition">Definition</h2>

<p>Convolution can be described by the so-called convolution <em>summation</em>. The convolution summation is pretty simple, and is defined as follows:</p>

<script type="math/tex; mode=display">y[i] = \sum_{j=0}^{M-1}h[j]x[i-j]</script>

<p>Where the length of the output signal <script type="math/tex">y[n]</script> is defined as <script type="math/tex">M + N - 1</script> where <script type="math/tex">M</script> is the length of the unit impulse response and <script type="math/tex">N</script> is the length of the input signal.</p>

<p>All this says is that a given sample <script type="math/tex">y[i]</script> in the output signal <script type="math/tex">y[n]</script> is determined by the summation of every <script type="math/tex">i^{th}</script> sample in every resultant impulse response. In effect, the summation above encodes how different samples in the resulting impulse responses contribute to a single output sample.</p>

<p>Natural imperative instinct might lead you to conclude that this can be easily implemented using nested iterations and arrays:</p>

<p>``` cpp
const int outputLength = M + N - 1;
int *y = new int<a href="">outputLength</a>;</p>

<p>for (int i = 0; i &lt; outputLength; ++i) {
  for (int j = 0; j &lt; M; ++j) {
    if (i - j &gt;= 0) y[i] += x[i - j] * h[j];
  }
}
```</p>

<p>But wait up! We are using Haskell, a functional programming language which typically does without both arrays and iteration. This means that to implement convolution in Haskell without the use of <a href="http://hackage.haskell.org/package/array">Arrays</a> or imperative iteration loops, we need to really understand the operation occurring in the convolution summation.</p>

<h3 id="convolution-machine">Convolution Machine</h3>

<p>The book <a href="http://www.dspguide.com">The Scientist and Engineer’s Guide to Digital Signal Processing</a> uses a metaphor known as the <a href="http://www.dspguide.com/ch6/4.htm">Convolution Machine</a> to help conceptualize the convolution operation at a granular level. The convolution machine is simply a theoretical machine in which the unit impulse response is:</p>

<ol>
  <li>wrapped onto a roller/cylinder</li>
  <li>rolled over the input signal such that each sample lines up with one on the reversed impulse response</li>
  <li>each lined-up pair of samples from input signal and impulse response is multiplied and each product is summed</li>
</ol>

<p>If you’re wondering why step <strong>3</strong> mentions a <em>reversed</em> impulse response, imagine that you have a roller and that the impulse response is on a strip of tape. Now imagine that you apply the impulse response tape over and around the roller, such that the numbers are facing you and are in the correct order. Now, when you roll this roller over and across the input signal, from left to right, the numbers on the impulse response tape will make contact with the input signal in <em>reverse order</em>.</p>

<p>See <a href="http://www.dspguide.com/ch6/4.htm">this page</a> for an illustration of the convolution machine in Figure 6-8.</p>

<h2 id="implementation">Implementation</h2>

<p>Implementing the convolution machine is pretty straightforward once we are able to conceptualize what it is actually doing.</p>

<p>Let’s start with the type signature. Since we’re not using arrays, we’ll represent the signals as lists of numbers. Convolution does something with two signals to produce a third signal, so the type signature is pretty straightforward:</p>

<p><code>haskell
convolve :: (Num a) =&gt; [a] -&gt; [a] -&gt; [a]
convolve hs xs = undefined
</code></p>

<p>In the signature, <code>xs</code> refers to the input signal and <code>hs</code> refers to the impulse response.</p>

<h3 id="padding">Padding</h3>

<p>Now for the implementation of <code>convolve</code>. First, consider this component of the convolution summation:</p>

<script type="math/tex; mode=display">x[i-j]</script>

<p>When we are computing the first sample, such that <script type="math/tex">i = 0</script>, in the output signal <script type="math/tex">y[n]</script>, then at one point we need to refer to the <script type="math/tex">x[-(M-1)]</script> sample where <script type="math/tex">M</script> is length of impulse response. However, there are no samples to the left of the first sample.</p>

<p>So what we have to do is prepad the input signal with <script type="math/tex">M-1</script> samples of value <script type="math/tex">0</script>. This padding has the added benefit of allowing us to simply map over the padded input signal to generate the output signal. This is because the convolution operation’s output signal length is <script type="math/tex">M + N - 1</script> where <script type="math/tex">M</script> is the length of the impulse response and <script type="math/tex">N</script> is the length of the input signal. The padding can be achieved with:</p>

<p><code>haskell
let pad = replicate ((length hs) - 1) 0
    ts  = pad ++ xs
</code></p>

<p>Once we prepad the input signal with enough zero samples, we can pass the padded input signal and impulse response to a function which simulates the rolling of the convolution machine. This function will be nested within <code>convolve</code> and will simply be used as a recursive helper function.</p>

<h3 id="lets-roll">Let’s Roll</h3>

<p><code>haskell
roll :: (Num a) =&gt; [a] -&gt; [a] -&gt; [a]
roll _  [] = []
roll hs ts = undefined
</code></p>

<p>The <code>roll</code> function is recursive and will simulate the actual rolling of the convolution machine over the input signal. As it rolls, it will consume the <code>head</code> of the input signal <code>ts</code>. Think of the consumption of the <code>head</code> as if the input signal is being wrapped around the roller as it rolls. The input signal <code>ts</code> will therefore eventually be empty, meaning the convolution machine has finished rolling over the entire input signal.</p>

<p>The <code>roll</code> function is run for every sample in the output signal. This is where the bulk of the implementation comes in. At any given sample in the input signal, we simulate the roll by zipping the input signal from that sample forward along with the impulse response. This generates a list of pairs each consisting of the input signal sample with its corresponding impulse response sample (which is being rolled over it).</p>

<p>If you have trouble conceptualizing this, imagine that the impulse response on the roller is tape, so that when you roll it over the input signal, the impulse response – which, remember, makes contact with the input signal in reverse – sticks to the input signal and is lined up such that each sample in the impulse response is directly over a sample of the input signal.</p>

<p>We then need to multiply the components of each pair with each other, i.e. the input sample multiplied by its corresponding impulse response sample. The act of zipping and multiplying the zipped up pairs can be done in one go with <code>zipWith (*)</code>. We then gather all of these products and <code>sum</code> them up. This sum is the latest computed sample in the output signal.</p>

<p>We construct the complete output signal by cons’ing the sample with a recursive call to <code>roll</code>, however this <code>roll</code> will concern only the next sample forward, thereby simulating rolling across the input signal.</p>

<p>With this information, we can finish the definition of <code>roll</code>:</p>

<p><code>haskell
roll :: (Num a) =&gt; [a] -&gt; [a] -&gt; [a]
roll _  [] = []
roll hs ts = let sample = sum $ zipWith (*) ts hs
             in sample : roll hs (tail ts)
</code></p>

<p>Here is the whole convolution function <code>convolve</code> put together:</p>

<p><code>haskell naive convolution in Haskell through the convolution machine
convolve :: (Num a) =&gt; [a] -&gt; [a] -&gt; [a]
convolve hs xs =
  let pad = replicate ((length hs) - 1) 0
      ts  = pad ++ xs
  in roll ts (reverse hs)
  where
    roll :: (Num a) =&gt; [a] -&gt; [a] -&gt; [a]
    roll _  [] = []
    roll hs ts = let sample = sum $ zipWith (*) ts hs
                 in sample : roll hs (tail ts)
</code></p>

<h2 id="reduction">Reduction</h2>

<p>Now that we understand the concept behind convolution, we can reduce the above implementation a bit further.</p>

<p>The observation we should make is that the <code>roll</code> function acts like <code>map</code>, specifically over <code>ts</code>. The only detail is that on every element mapped over, the result of that element’s mapping concerns the list <code>ts</code> from that element forward. If we are on the third element of <code>ts</code>, we only act on the third element forward. In other words, we are mapping over every <code>tail</code> of <code>ts</code>. Knowing this, we can change the <code>roll</code> function to a straight up <code>map</code> over <code>tails ts</code>.</p>

<p>However, <code>tails</code> considers <code>[]</code> to be a tail of any list – which is technically correct – so we’ll always have a trailing <code>0</code> element if we do it this way. That’s why we simply take the <code>init</code> of the result of <code>tails</code>, which returns every element in a list except the last one. We also still need to prepad the signal, so those lines remain:</p>

<p><code>haskell a reduced form of the convolution machine implementation
convolve :: (Num a) =&gt; [a] -&gt; [a] -&gt; [a]
convolve hs xs =
  let pad = replicate ((length hs) - 1) 0
      ts  = pad ++ xs
  in map (sum . zipWith (*) (reverse hs)) (init $ tails ts)
</code></p>

<h2 id="parallelization">Parallelization</h2>

<p>There’s something to be said about how the various properties of the Haskell language come together to make certain algorithms trivially parallelizable. Green threads, single assignment, function purity and its consequent idempotence/referential transparency – I can go on and on, but I’d rather not digress from the topic of this post. You won’t get any Monad koolaid from me. Still, I think it’s interesting to note how easy it can be to parallelize this naive convolution algorithm. So let’s do it.</p>

<h3 id="parmap">parMap</h3>

<p>The <a href="http://hackage.haskell.org/package/parallel">parallel</a> Haskell package contains various tools for parallelization. One of these is the <a href="http://hackage.haskell.org/packages/archive/parallel/latest/doc/html/Control-Parallel-Strategies.html">Control.Parallel.Strategies</a> module, which defines the <a href="http://hackage.haskell.org/packages/archive/parallel/latest/doc/html/Control-Parallel-Strategies.html#v:parMap"><code>parMap</code></a> function, which maps over list elements in parallel, in essence, a parallel map:</p>

<p><code>haskell
parMap :: Strategy b -&gt; (a -&gt; b) -&gt; [a] -&gt; [b]
</code></p>

<p><code>parMap</code> takes an <a href="http://hackage.haskell.org/packages/archive/parallel/latest/doc/html/Control-Parallel-Strategies.html#t:Strategy">evaluation strategy</a> which is used to actually perform the evaluation in parallel. We use the <a href="http://hackage.haskell.org/packages/archive/parallel/latest/doc/html/Control-Parallel-Strategies.html#v:rdeepseq"><code>rdeepseq</code></a> evaluation strategy, which fully evaluates the argument to Normal Form (i.e. fully evaluated), as opposed to <a href="http://hackage.haskell.org/packages/archive/parallel/latest/doc/html/Control-Parallel-Strategies.html#v:rseq"><code>rseq</code></a> which merely evaluates the argument to <a href="http://en.wikibooks.org/wiki/Haskell/Graph_reduction#Weak_Head_Normal_Form">Weak Head Normal Form</a> (<abbr title="Weak Head Normal Form">WHNF</abbr>). The <code>rdeepseq</code> strategy can only operate on arguments it knows it can fully evaluate, those that conform to the <a href="http://hackage.haskell.org/packages/archive/deepseq/latest/doc/html/Control-DeepSeq.html#t:NFData"><code>NFData</code></a> typeclass from the <a href="http://hackage.haskell.org/package/deepseq">Control.Deepseq</a> module. To conform to this, we add another type constraint to our convolution parameters:</p>

<p><code>haskell
parConvolve :: (NFData a, Num a) =&gt; [a] -&gt; [a] -&gt; [a]
</code></p>

<p>Continuing forward, all we have to do now is make a drop-in replacement of <code>map</code> with <code>parMap</code>. Actually, it’s not quite a drop-in replacement, because we need to supply <code>parMap</code> with the <code>rdeepseq</code> evaluation strategy:</p>

<p><code>haskell a parallelized version of the reduced naive convolution algorithm
parConvolve :: (NFData a, Num a) =&gt; [a] -&gt; [a] -&gt; [a]
parConvolve hs xs =
  let pad = replicate ((length hs) - 1) 0
      ts  = pad ++ xs
  in parMap rdeepseq (sum . zipWith (*) (reverse hs)) (init $ tails ts)
</code></p>

<h3 id="benchmark">Benchmark</h3>

<p>The <a href="http://hackage.haskell.org/packages/archive/criterion"><code>criterion</code></a> Haskell package provides tools for benchmarking and analyzing code. The synthetic benchmark we will conduct will run each implementation with an impulse response of length 100 and an input signal of length 1,000.</p>

<p>In the following code, <code>conv</code> is the naive implementation, <code>conv'</code> is the reduced naive implementation, and <code>parConv</code> is the parallel implementation:</p>

<p>``` haskell criterion benchmarking code
data ConvType = Naive | Reduced | Parallel deriving (Eq, Ord)
convTypes = Data.Map.fromList [(Naive, conv), (Reduced, conv’), (Parallel, parConv)]</p>

<p>main = defaultMain [
  bench “Naive Convolution” (runConv Naive),
  bench “Reduced Convolution” (runConv Reduced),
  bench “Parallelized Convolution” (runConv Parallel) ]
  where runConv ctype =
          let hs = [1..100 :: Int]
              ts = [1..1000 :: Int]
              convfn = fromJust $ Data.Map.lookup ctype convTypes
          in nf (convfn hs) ts
```</p>

<p>Compile the benchmark with:</p>

<p><code>bash
$ ghc --make -O2 -threaded -o conv conv.hs
</code></p>

<p>Run it with:</p>

<p><code>bash
$ ./conv -o bench.html -r out.csv +RTS -N4
</code></p>

<p>The <code>-o</code> parameter specifies an output file for generated <a href="../../../../static/html/convolution-criterion.html">charts and graphs</a>. The <code>-r</code> parameter specifies a comma separated value (<abbr title="Comma Separated Value">CSV</abbr>) file to output relative statistics which we use to measure performance relative to the reference, non-reduced naive implementation.</p>

<p>The <code>+RTS</code> parameter is a delimiter which begins parameters to the <a href="http://www.haskell.org/ghc/docs/latest/html/users_guide/runtime-control.html">runtime system</a>. The <code>-N#</code> parameter specifies how many cores to utilize. The machine I was using has 6 cores, but I found that using less than that lowered the amount of statistical variance. I imagine this was because the computer was able to continue its own tasks on the other two cores.</p>

<p>The above benchmark yielded the following results:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Name</th>
      <th style="text-align: center">% faster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Naive (Reference)</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: left">Reduced</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: left">Parallel</td>
      <td style="text-align: center">54</td>
    </tr>
  </tbody>
</table>

<p>The parallel version apparently really boosts performance. An important thing to realize is that when parallelizing things, it’s considered best to only parallelize when the benefits outweigh the relative overhead of managing the green threads.</p>

<p>For example, in my tests, changing the impulse response length to 5 and the input signal length to 10 shows the parallel version to be 27% slower than the naive implementation. Also notice that the reduced version is a bit slower, for what I can only imagine to be a <abbr title="Glasgow Haskell Compiler">GHC</abbr> optimization that applies to the naive implementation but not to the reduced version.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Name</th>
      <th style="text-align: center">% faster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Naive (Reference)</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: left">Reduced</td>
      <td style="text-align: center">-16</td>
    </tr>
    <tr>
      <td style="text-align: left">Parallel</td>
      <td style="text-align: center">-27</td>
    </tr>
  </tbody>
</table>

<p>On the other hand, increasing the impulse response length to 1,000 and the input signal length to 10,000 maintained a similar performance increase:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Name</th>
      <th style="text-align: center">% faster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Naive (Reference)</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: left">Reduced</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: left">Parallel</td>
      <td style="text-align: center">48</td>
    </tr>
  </tbody>
</table>

<h2 id="conclusion">Conclusion</h2>

<p>I’m new to Digital Signal Processing, so if you notice any glaring errors please feel free to correct me; I would appreciate it. If you are interested in this subject and would like to read a book to learn more, I wholeheartedly recommend <a href="http://www.dspguide.com">The Scientist and Engineer’s Guide to Digital Signal Processing</a>. If you would like to learn more about Convolution, you can check the relevant chapters in that freely available book.</p>

<h3 id="imperative-approach">Imperative Approach</h3>

<p>You can also check out <a href="http://www.songho.ca/dsp/convolution/convolution.html">this page</a> as well, which also covers multidimensional convolution with a concrete example of a <a href="http://en.wikipedia.org/wiki/Gaussian_filter">Gaussian filter</a> applied to an image for the purposes of blurring it. This specific application of the Gaussian filter is known as the <a href="http://en.wikipedia.org/wiki/Gaussian_blur">Gaussian Blur</a>. The Gaussian Blur is pretty popular in realtime image rendering, such as in video games, because of a property it has which allows it be applied in two dimensions, e.g. in an image, as two independent one-dimensional operations. This makes it dramatically faster and more efficient, and is trivial to implement in modern <abbr title="Graphics Processing Unit">GPU</abbr> <a href="http://www.opengl.org/wiki/Compute_Shader">Compute shaders</a> <sup id="fnref:compute_shader"><a href="#fn:compute_shader" rel="footnote">1</a></sup>. Such shaders can then be used to implement effects such as motion blur in games <sup id="fnref:motion_blur"><a href="#fn:motion_blur" rel="footnote">2</a></sup>. The page also provides imperative implementations of convolution in C++.</p>

<h3 id="optimizations">Optimizations</h3>

<p>Haskell is known for having many ways of doing any one thing, so if you come up with a better solution feel free to <a href="https://gist.github.com">gist it</a> and post it in the comments.</p>

<p>Of course, this post concerns a <em>naive</em> implementation of convolution. There are other more optimized implementations of convolution, such as <abbr title="Fast Fourier Transform">FFT</abbr> convolution which exploits the Fast Fourier Transform and the principle of duality – convolution in the time domain is equivalent to multiplication in the frequency domain – to perform convolution a lot faster in some cases.</p>

<div class="footnotes">
  <ol>
    <li id="fn:compute_shader">
      <p>As described in <a href="http://www.d3dcoder.net/d3d11.htm">3D Game Programming with DirectX 11</a> by Frank D. Luna in Chapter 12, page 450, § 12.7<a href="#fnref:compute_shader" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:motion_blur">
      <p>Despite this optimization of Gaussian Blurring, many implementations optimize further. Blurring typically involves rendering the scene to a separate buffer (e.g. Render-to-Texture) at a scaled-down resolution. This speeds up the blurring operation as there are less pixels to operate on. Then the result is rendered to the actual screen. Since the point is to blur, the upscaling is usually hardly noticeable.</p>

      <p>Recently I purchased an old game on Steam which I had played circa 2003. This game was developed back when 1280x1024 was a popular resolution, that is 4:3 aspect ratio. I got to a part where the game displayed some sort of blur effect and noticed that the entire screen was completely blurred to the point where I couldn’t make anything out. I presume this was not the intended effect. If I had to guess, I imagine they hard-coded a scaled down resolution – and thus aspect ratio as well – at which to render the scene for blurring, such that upscaling it to my current 1920x1080 resolution 16:9 AR looked horrible. I imagine newer games take into account aspect ratio and some other factor to scale down the current resolution from.<a href="#fnref:motion_blur" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I'm Back]]></title>
    <link href="http://blaenkdenum.com/posts/im-back/"/>
    <updated>2012-12-25T23:25:00-08:00</updated>
    <id>http://blaenkdenum.com/posts/im-back</id>
    <content type="html"><![CDATA[<p>It’s been a while since I last had a blog. In fact, I think it’s been two, maybe three years now. I previously had a WordPress blog that I updated regularly. However, the things I was studying on my own at the time consumed me and I considered writing about those things to be a low priority. After all, who want’s to hear what a noob thinks about something he’s barely beginning to learn about? So instead I figured I’d continue reading and studying, and maintaining a blog kind of fell into the backburner.</p>

<h3 id="blogging">Blogging</h3>

<p>For over a year now I had wanted to get back into blogging, at the very least to serve as a sort of journal for what I’ve learned, what I’m working on, etc. I didn’t want to bother maintaining a WordPress blog anymore, so I gave tumblr a shot. I felt that tumblr wasn’t as flexible as I would like and lost interest again. A few months ago I got the urge again to set up a blog, and having heard of static site generators on hacker news, I ended up giving Jekyll/Octopress a try. That’s what I’m running right now and so far it’s working out well. I actually set this up a few months ago, but hadn’t had the time to properly set it up and design myself a theme.</p>

<h3 id="haskell">Haskell</h3>

<p>I’ve done a lot during my absence these past few years. In fact, it’s all a big blur so I don’t know where to begin to talk about it. Most recently, for example, I worked on a <a href="https://github.com/blaenk/pulse-visualizer">music visualizer</a> for Linux’ Pulse Audio in Haskell using OpenGL and FFTW. This was the first program I wrote in Haskell, aside from little programs I would write to learn the language. I learned Haskell primarily using a combination of <a href="http://learnyouahaskell.com/">Learn You a Haskell for Great Good</a> and <a href="http://book.realworldhaskell.org/read/">Real World Haskell</a>. For certain things, such as understanding the concept of laziness in Haskell, I used the <a href="http://en.wikibooks.org/wiki/Haskell/Laziness">Haskell Wikibook</a>. I also had a Professor at school who was familiar with Haskell and DSP as my mentor. Thanks to his previous experience, I was able to learn both things pretty quickly. Whenever I was unsure of my understanding of a given topic, such as Monads, I had him to discuss and refine my understanding with.</p>

<h3 id="digital-signal-processing">Digital Signal Processing</h3>

<p>I worked on this project to learn both Haskell and Digital Signal Processing. The book I used for learning Digital Signal Processing is <a href="http://www.dspguide.com/pdfbook.htm">The Scientist and Engineer’s Guide to Digital Signal Processing</a>. For the longest time – probably the same amount of time I’ve been without a blog – I’ve wanted to learn about Digital Signal Processing, but every book I found assumed some prior DSP knowledge, electronics/circuits knowledge, or hardcore mathematical understanding. This book seems to teach DSP from the perspective of a developer, showing code for algorithms discussed throughout, such as the <a href="http://en.wikipedia.org/wiki/Fast_Fourier_transform">Fast Fourier Transform</a>.</p>

<p>I will probably write a post with more information about the project and what it was like to learn Haskell and DSP, and where I intend to go forward with this. For now I just wanted to push out a real post, unlike the <a href="/posts/test-post/">test post</a> which I use to test the design and markup of this site.</p>
]]></content>
  </entry>
  
</feed>
